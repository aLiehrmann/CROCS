---
title: "CROCS-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CROCS-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## CROCS for supervised segmentation procedure on ChIP-Seq data

```{r}
library(CROCS)
library(data.table)
library(purrr)
library(furrr)
library(PeakError)
library(penaltyLearning)
```

* To run the code, the user will need to provide a `data.table` object `counts` which contains informations about the normalized coverages, a `data.table` object `regions` wich contains informations about the labels and a `data.table` object `peakBounds` which contains informations about the upper bound and lower bound on the true number of peaks for each labeled coverage profiles used in the training set. Standard used for labeling ChIP-seq signals is described in [Optimizing ChIP-seq peak detectors using visual labels and supervised machine learning, Hocking et al. 2017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408812/).  
* `counts` needs to have at least the following columns:
  + `counts$sample.id`: a unique identifier for each biological source which allows you to learn the tuning parameters from several biological replicates ;
  + `counts$chunk`: a unique identifier for each genomic chunk (several genomic chunks by biological source) ;
  + `counts$coverage`: the normalized coverage ;
  + `counts$chromStart` & `counts$chromEnd`: the genomic start and end position of a series of observations associated with the same normalized coverage value. This compressed format allows to reduce the running time of the models computation.
* `regions` needs to have at least the following columns:
  + `regions$sample.id`: same as `counts$sample.id` ;
  + `regions$chunk`: same as `counts$chunk` ;
  + `regions$chromStart` & `counts$chromEnd`: the genomic start and end position of the label (several labels by chunk) ;
  + `regions$annotation`: the type of the label.
* `peakBounds` need to have at least the following columns:
  + `peakBounds$sample.id`: same as `counts$sample.id` ;
  + `peakBounds$chunk`: same as `counts$chunk` ;
  + `peakBounds$lower_bound_peak` & `peakBounds$upper_bound_peak`: an lower and upper peak bound between wich `CROCS::CROCS` will retreive all optimal reachable segmentations. The choice of these two bounds is specific to each problem. We advise the user to choose the bounds so that there is a segmentation with at least one false positive label and another with one false negative label.
* **WARNING**: An important condition for learning a good penalty $\lambda$ and dispersion $\phi$ values is that the length of the genomic chunks used in the training set is similar to the length of the genomic chunks used in the validation set. 
* In addition to providing the `counts`, `regions` and `peakBounds` tables, the user needs to specify: the number of threads `nb_threads` that will be used during the analysis (it signifcantly reduces the running time of the models computation); the identifiers of the chunks that will be used in the training set `training_chunks` and the those used in the validation set `test_chunks`; the noise assumption `my_loss_type`; the transformation `my_transformation`; the peak shape assumption `my_graph` and the peak start/end post-processing rule `my_post_processsing_rule` that will be used to build his peak caller `my_peak_caller`. If the chosen noise model is the negative binomial `"negbin"`, the user needs to provide a grid of `phi` on which segmentation models will be computed (see Example 2). 

```{r}
nb_threads <- 20
training_chunks <- 1:3
test_chunks <- 4
my_graph <- "std"
my_loss_type <- "mean"
my_transformation <- "anscombe_poisson"
my_post_processing_rule <- "maxjump"
head(regions <- as.data.table(CROCS::regions))
head(counts <- as.data.table(CROCS::counts))
peakBounds <- unique(regions[,list(sample.id, chunk)])
peakBounds[, lower_bound_peak := 1]
peakBounds[, upper_bound_peak := 9]
```

We split the genomic chunks into training and validation sets.

```{r}
plan(multisession, workers=nb_threads)
training_counts <- counts[chunk %in% training_chunks,]
test_counts <- counts[chunk %in% test_chunks,]
training_regions <- regions[chunk %in% training_chunks,]
test_regions <- regions[chunk %in% test_chunks,]
```

We compute machine learning features on the problems. These features will be use latter in the suprevised learning procedure. 

```{r}
all_features <- as.data.table(featureMatrix(
    data.sequences = counts, 
    problem.vars = c("sample.id","chunk"), 
    data.var = "coverage"
), keep.rownames = T)
all_features <- data.table(cbind(all_features, unique(counts[,.(sample.id,chunk)])))
setkey(all_features,chunk,sample.id)
training_features <- all_features[unique(training_counts[,.(chunk,sample.id)])]
test_features <- all_features[unique(test_counts[,.(chunk,sample.id)])]
```

### Example 1: the unconstrained segmentation model with gaussian transformed noise and max-jump post-processing rule as peak caller

We define the peak caller.

```{r}
std_g <- graphFactory(graph=my_graph)
loss <- lossFactory(type=my_loss_type)
tr <- transformationFactory(transformation=my_transformation)
rule <- postProcessingRuleFactory(rule=my_post_processing_rule)

my_peak_caller <- peakCallerFactory(
  mygraph=std_g, 
  transformation_f=tr, 
  loss_f=loss,
  postProcessingRule_f=rule
) 
```

For each problem in the training set we compute all optimal models between 1 and 9 peaks with the help of `CROCS::CROCS`. The choice of the lower and upper peak bound is problem-specific.

```{r}
training_models <- future_map(
  split(training_counts, by=c("chunk", "sample.id"), drop = TRUE),
  function(counts_p){
    library(data.table, quietly=TRUE, warn.conflicts=FALSE)
    y <- counts_p$coverage
    w <- counts_p$chromEnd - counts_p$chromStart
    peakBounds_c <- peakBounds[chunk==counts_p$chunk[[1]] & sample.id == counts_p$sample.id[[1]],]
    models <- CROCS(
      data=y, 
      weights=w, 
      lower_bound_peak=peakBounds_c$lower_bound_peak,
      upper_bound_peak=peakBounds_c$upper_bound_peak,
      solver=my_peak_caller
    )
    map(models, function(model_p){
      if (is.na(model_p$start[[1]])){
        model_p$chromStart <- 0
        model_p$chromEnd <- 0.1
      } else {
        model_p$chromStart <- counts_p$chromEnd[model_p$start]
        model_p$chromEnd <- counts_p$chromEnd[model_p$end]
      }
      model_p
    })
  }, 
  .options = furrr_options(seed=NULL)
)
print(cbind(
  chromStart = training_models[[1]][[8]]$chromStart,
  chromEnd = training_models[[1]][[8]]$chromEnd
))
```

Given the labels of each problem we compute the errors associated to each model previously computed.

```{r}
training_errors <- future_map(
  split(training_regions, by=c("chunk", "sample.id"), drop=TRUE), 
  function(region_p){
    map(
      training_models[[paste0(region_p$chunk[[1]],".",region_p$sample.id[[1]])]], 
      function(model_p){
        peaks <- data.frame(
          chromStart=model_p$chromStart, 
          chromEnd=model_p$chromEnd
        )
        errors <- PeakErrorChrom(peaks=peaks, regions=region_p)
        errors$changepoints <- model_p$changepoints
        errors
      }
    )
  },
  .options = furrr_options(seed=NULL)
)
```

We merge the computed models and associated errors.

```{r}
training_models_f <- rbindlist(pmap(
  unique(counts[,.(sample.id,chunk)]), 
  function(sample.id, chunk){
    rbindlist(map(
      training_models[[paste0(chunk,".", sample.id)]], 
      function(model_p){
        data.table(
          chunk=chunk,
          sample.id=sample.id,
          changepoints=model_p$changepoints,
          loss=model_p$loss
        )
      }
    ))
  }
))
head(training_models_f)
training_errors_f <- rbindlist(pmap(
  unique(counts[,.(sample.id,chunk)]), 
  function(sample.id, chunk){
    rbindlist(map(
      training_errors[[paste0(chunk,".", sample.id)]], 
      function(errors_p){
        data.table(
          chunk=chunk,
          sample.id=sample.id,
          changepoints=errors_p$changepoints,
          errors=sum(errors_p$fn+errors_p$fp),
          labels=nrow(errors_p)
        )
      }
    ))
  }
))
head(training_errors_f)
dt_model_error <- merge(
  x = training_models_f, 
  y = training_errors_f, 
  by=c("chunk", "sample.id", "changepoints")
)
head(dt_model_error)
```

We compute the piecewise constant objective function $E(\lambda)$ for each problem.

```{r}
training_modelSelection <- dt_model_error[,modelSelection(
  models = .SD,
  loss = "loss",
  complexity = "changepoints"
), by=.(chunk, sample.id)]
head(training_modelSelection)
```

We search the problem-specific interval of $\lambda$ associated with minimum error.

```{r}
training_targets <- targetIntervals(
  models = training_modelSelection, 
  problem.vars = c("chunk", "sample.id")
)
head(training_targets)
```

We learn a function that predicts problem-specific $\log(\lambda)$ values.

```{r}
training_targets <- training_targets[order(chunk, sample.id)]
training_features <- training_features[order(chunk, sample.id)]
training_targets_mat <- as.matrix(
  training_targets[ ,.(min.log.lambda, max.log.lambda)]
)
training_features_mat <- as.matrix(
  training_features[,!c("rn", "chunk", "sample.id")]
)
training_features_mat <- training_features_mat[
  is.finite(training_targets_mat[,1])|is.finite(training_targets_mat[,2]),
]
training_targets_mat <- training_targets_mat[
  is.finite(training_targets_mat[,1])|is.finite(training_targets_mat[,2]),
]
set.seed(2020)
fold.vec = sample(rep(1:4, l = nrow(training_features_mat)))
lambda_function <- IntervalRegressionCV(
  target.mat = training_targets_mat,
  feature.mat = training_features_mat,
  LAPPLY = future_map,
  fold.vec = fold.vec
)
```

We predict $\lambda$ values for each problem from the validation set or unlabeled problems.

```{r}
predicted_log_lambda <- lambda_function$predict(
  as.matrix(test_features[,!c("rn", "chunk", "sample.id")])
)
predicted_lambda <- data.table(
  chunk=test_features$chunk,
  sample.id=test_features$sample.id,
  lambda=exp(predicted_log_lambda[,1])
)
head(predicted_lambda)
```

Using previously computed problem-specific $\lambda$ values, we retreive peaks associated to each problem from the validation set or unlabeled problems.

```{r}
split_test_counts <- split(test_counts, by=c("chunk", "sample.id"),drop=TRUE)
predicted_models <- pmap(predicted_lambda, function(chunk, sample.id, lambda){
  current_pb <- split_test_counts[[paste0(chunk,".",sample.id)]]
  y <- current_pb$coverage
  w <- current_pb$chromEnd - current_pb$chromStart
  predicted_model <- my_peak_caller(
    data = y,
    weights = w,
    lambda = lambda
  )
  if (is.na(predicted_model$start[[1]])){
    predicted_model$chromStart <- 0
    predicted_model$chromEnd <- 0.1
  } else {
    predicted_model$chromStart <- current_pb$chromEnd[predicted_model$start]
    predicted_model$chromEnd <- current_pb$chromEnd[predicted_model$end]
  }
  predicted_model
})
names(predicted_models) <- paste0(
  predicted_lambda$chunk,
  ".",
  predicted_lambda$sample.id
)
names(predicted_models)[[3]]
print(cbind(
  chromStart = predicted_models[[3]]$chromStart,
  chromEnd = predicted_models[[3]]$chromEnd
))
```

Since we have labels for the validation set, we can compute the accuracy.

```{r}
predicted_errors <- rbindlist(future_map(
  split(test_regions, by=c("chunk", "sample.id"), drop=TRUE), 
  function(region_p){
    model <- predicted_models[[paste0(region_p$chunk[[1]],".",region_p$sample.id[[1]])]]
    peaks <- data.frame(
      chromStart=model$chromStart, 
      chromEnd=model$chromEnd
    )
    errors <- PeakErrorChrom(peaks=peaks, regions=region_p)
    errors
  },
  .options = furrr_options(seed=NULL)
))
print(paste0(1-predicted_errors[,sum(fp+fn)/.N]," accuracy"))
```

```{r}
plan(sequential)
``` 

### Example 2: the unconstrained segmentation model with negative binomial noise and max-jump post-processing rule as peak caller

```{r}
my_graph <- "std"
my_loss_type <- "negbin"
my_transformation <- "raw"
my_post_processsing_rule <- "maxjump"
phi <- exp(seq(log(1),log(1000),length=8))
``` 

We define the peak caller.

```{r}
plan(multisession, workers=nb_threads)
std_g <- graphFactory(graph=my_graph)
loss <- lossFactory(type=my_loss_type)
tr <- transformationFactory(transformation=my_transformation)
rule <- postProcessingRuleFactory(rule=my_post_processing_rule)

my_peak_caller <- peakCallerFactory(
  mygraph=std_g, 
  transformation_f=tr, 
  loss_f=loss,
  postProcessingRule_f=rule
) 
```

For each problem in the training set we compute all optimal models between 1 and 9 peaks with the help of `CROCS::CROCS`. The choice of the lower and upper peak bound is problem-specific. In order to calibrate the dispersion $\phi$ parameter from the negative binomial distribution, we repeate this step for a range of $\phi$ values.

```{r}
training_models <- future_map(
  split(training_counts, by=c("chunk", "sample.id"), drop=TRUE),
  function(counts_p){
    map(phi, function(phi_p){
      library(data.table, quietly=TRUE, warn.conflicts=FALSE)
      y <- counts_p$coverage
      w <- counts_p$chromEnd - counts_p$chromStart
      peakBounds_c <- peakBounds[chunk==counts_p$chunk[[1]] & sample.id == counts_p$sample.id[[1]],]
      models <- CROCS(
        data=y, 
        weights=w, 
        lower_bound_peak=peakBounds_c$lower_bound_peak, 
        upper_bound_peak=peakBounds_c$upper_bound_peak,
        solver=my_peak_caller,
        phi=phi_p
      )
      map(models, function(model_p){
        if (is.na(model_p$start[[1]])){
          model_p$chromStart <- 0
          model_p$chromEnd <- 0.1
        } else {
          model_p$chromStart <- counts_p$chromEnd[model_p$start]
          model_p$chromEnd <- counts_p$chromEnd[model_p$end]
        }
        model_p
      })
    })
  }, 
  .options = furrr_options(seed=NULL)
)
```

Given the labels of each problem we compute the errors associated to each model previously computed.

```{r}
training_errors <- future_map(
  split(training_regions, by=c("chunk", "sample.id"), drop=TRUE), 
  function(region_p){
    map(
      training_models[[paste0(region_p$chunk[[1]],".",region_p$sample.id[[1]])]], 
      function(phi_p){
        map(phi_p, function(model_p){
          peaks <- data.frame(
            chromStart=model_p$chromStart, 
            chromEnd=model_p$chromEnd
          )
          errors <- PeakErrorChrom(peaks=peaks, regions=region_p)
          errors$changepoints <- model_p$changepoints
          errors$phi <- model_p$phi
          errors
        })
      }
    )
  },
  .options = furrr_options(seed=NULL)
)
```

We merge the computed models and associated errors.

```{r}
# /// compute lambda function
training_models_f <- rbindlist(future_pmap(
  unique(counts[,.(sample.id,chunk)]), 
  function(sample.id, chunk){
    rbindlist(map(
      training_models[[paste0(chunk,".", sample.id)]],
      function(phi_p){
        rbindlist(map(phi_p, 
          function(model_p){
            data.table(
              chunk=chunk,
              sample.id=sample.id,
              changepoints=model_p$changepoints,
              loss=model_p$loss,
              phi=model_p$phi
            )
          }
        ))
      }
    ))
  }
))
head(training_models_f)
training_errors_f <- rbindlist(future_pmap(
  unique(counts[,.(sample.id,chunk)]), 
  function(sample.id, chunk){
    rbindlist(map(
      training_errors[[paste0(chunk,".", sample.id)]],
      function(phi_p){
        rbindlist(map(phi_p, 
          function(errors_p){
            data.table(
              chunk=chunk,
              sample.id=sample.id,
              changepoints=errors_p$changepoints,
              phi=errors_p$phi,
              errors=sum(errors_p$fn+errors_p$fp),
              labels=nrow(errors_p)
            )
          }
        ))
      }
    ))
  }
))
head(training_errors_f)
dt_model_error <- merge(
  x = training_models_f, 
  y = training_errors_f, 
  by=c("chunk", "sample.id", "changepoints","phi")
)
head(dt_model_error)
```

We compute the piecewise constant objective function $E(\lambda)$ for each problem and each $\phi$ value.

```{r}
training_modelSelection <- dt_model_error[,modelSelection(
  models = .SD,
  loss = "loss",
  complexity = "changepoints"
), by=.(chunk, sample.id, phi)]
head(training_modelSelection)
```

We search the $\phi$ value which minimizes the errors over all problems of the training set. 

```{r}
training_modelSelection_by_phi <- split(training_modelSelection, by="phi", drop=TRUE)
dt <- rbindlist(map(training_modelSelection_by_phi, function(dt_by_phi) {
  sorted_lambda <- sort(c(dt_by_phi$min.log.lambda, dt_by_phi$max.log.lambda))
  grid <- rle(sorted_lambda)$values
  errors <- vector(mode="integer", length= length(grid)-1)
  walk(1:nrow(dt_by_phi), function(i){
    a <- which(grid>=dt_by_phi$min.log.lambda[[i]] & grid<dt_by_phi$max.log.lambda[[i]])
    errors[a] <<- errors[a] + dt_by_phi$errors[[i]]
  })
  i_min <- which.min(errors)
  predicted.lambda <- sum(grid[i_min:(i_min+1)])/2
  data.table(phi = dt_by_phi$phi[[1]], errors = min(errors))
}))
print(predicted_phi <- dt$phi[which.min(dt$errors)])
```

After fixing $\phi$, we search the problem-specific interval of $\lambda$ associated with minimum error.

```{r}
training_targets <- targetIntervals(
  models = training_modelSelection[phi==predicted_phi], 
  problem.vars = c("chunk", "sample.id")
)
head(training_targets)
```

We learn a function that predicts problem-specific $\log(\lambda)$ values.

```{r}
training_targets <- training_targets[order(chunk, sample.id)]
training_features <- training_features[order(chunk, sample.id)]
training_targets_mat <- as.matrix(
  training_targets[ ,.(min.log.lambda, max.log.lambda)]
)
training_features_mat <- as.matrix(
  training_features[,!c("rn", "chunk", "sample.id")]
)
training_features_mat <- training_features_mat[
  is.finite(training_targets_mat[,1])|is.finite(training_targets_mat[,2]),
]
training_targets_mat <- training_targets_mat[
  is.finite(training_targets_mat[,1])|is.finite(training_targets_mat[,2]),
]
set.seed(2020)
fold.vec = sample(rep(1:4, l = nrow(training_features_mat)))
lambda_function <- IntervalRegressionCV(
  target.mat = training_targets_mat,
  feature.mat = training_features_mat,
  LAPPLY = future_map,
  fold.vec = fold.vec
)
```

We predict $\lambda$ values for each problem from the validation set or unlabeled problems.

```{r}
predicted_log_lambda <- lambda_function$predict(
  as.matrix(test_features[,!c("rn", "chunk", "sample.id")])
)
predicted_lambda <- data.table(
  chunk=test_features$chunk,
  sample.id=test_features$sample.id,
  lambda=exp(predicted_log_lambda[,1])
)
head(predicted_lambda)
```

Using previously computed problem-specific $\lambda$ values and the consensus $\phi$ value, we retreive peaks associated to each problem from the validation set or unlabeled problems.

```{r}
split_test_counts <- split(test_counts, by=c("chunk", "sample.id"), drop=TRUE)
predicted_models <- pmap(predicted_lambda, function(chunk, sample.id, lambda){
  current_pb <- split_test_counts[[paste0(chunk,".",sample.id)]]
  y <- current_pb$coverage
  w <- current_pb$chromEnd - current_pb$chromStart
  predicted_model <- my_peak_caller(
    data = y,
    weights = w,
    lambda = lambda,
    phi = predicted_phi
  )
  if (is.na(predicted_model$start[[1]])){
    predicted_model$chromStart <- 0
    predicted_model$chromEnd <- 0.1
  } else {
    predicted_model$chromStart <- current_pb$chromEnd[predicted_model$start]
    predicted_model$chromEnd <- current_pb$chromEnd[predicted_model$end]
  }
  predicted_model
})
names(predicted_models) <- paste0(
  predicted_lambda$chunk,
  ".",
  predicted_lambda$sample.id
)
```

Since we have labels for the validation set, we can compute the accuracy.

```{r}
predicted_errors <- rbindlist(future_map(
  split(test_regions, by=c("chunk", "sample.id"), drop=TRUE), 
  function(region_p){
    model <- predicted_models[[paste0(region_p$chunk[[1]],".",region_p$sample.id[[1]])]]
    peaks <- data.frame(
      chromStart=model$chromStart, 
      chromEnd=model$chromEnd
    )
    errors <- PeakErrorChrom(peaks=peaks, regions=region_p)
    errors
  },
  .options = furrr_options(seed=NULL)
))

print(paste0(1-predicted_errors[,sum(fp+fn)/.N]," accuracy"))
```

```{r}
plan(sequential)
``` 
